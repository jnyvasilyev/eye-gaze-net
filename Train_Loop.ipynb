{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWVAyg_GsMRN",
    "outputId": "fd891c81-410b-4020-df26-322a6eae0ba1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from ipdb import set_trace\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import model\n",
    "from warp import WarpImageWithFlowAndBrightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_list(json_list):\n",
    "    ldmks = [eval(s) for s in json_list]\n",
    "    return np.array([(x, y, z) for (x, y, z) in ldmks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_info(filename):\n",
    "    info_dict = {}\n",
    "    \n",
    "    # Get image data\n",
    "    img = cv2.cvtColor(cv2.imread(\"%s.jpg\" % filename[:-5]), cv2.COLOR_BGR2RGB)\n",
    "    # TODO: crop image\n",
    "    img = cv2.resize(img, (64, 32))\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    info_dict['img'] = img\n",
    "    \n",
    "\n",
    "    # Get ID data\n",
    "    titles_types = {'ID': int, 'T': str, 'N': int, 'F': int, 'V': float, 'H': float}\n",
    "    info_list = os.path.basename(filename[:-5]).split('_')\n",
    "    for title, info in zip(titles_types, info_list):\n",
    "        info_dict[title] = titles_types[title](info[len(title):])\n",
    "    info_dict['target'] = info_dict['F'] == 1\n",
    "        \n",
    "    # Get json data\n",
    "    json_data_file = open(filename)\n",
    "    json_data = json.load(json_data_file)\n",
    "    info_dict[\"look_vec\"] = np.array(eval(json_data[\"eye_details\"][\"look_vec\"])) # 3D look direction vector\n",
    "    info_dict[\"interior_margin\"] = process_json_list(json_data[\"interior_margin_2d\"]) # eye interior landmarks\n",
    "    info_dict[\"caruncle\"] = process_json_list(json_data[\"caruncle_2d\"]) # caruncle landmarks\n",
    "    info_dict[\"iris\"] = process_json_list(json_data[\"iris_2d\"]) # iris landmarks\n",
    "    \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the dataframe info\n",
    "- look_vec is a 3D homogeneous vector in the form \\[x, y, z, 0\\]. For purposes of the model input, only the x and y components are needed. I believe this vector is already normalized, but we can renormalize this vector before extracting the x and y components\n",
    "- Feature landmarks (i.e., interior_margin, caruncle, and iris) are the pixel coordinates of feature BEFORE RESIZING. Ideally they are used to determine the crop/resize area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "The above dataframe contains info on all images, separated into IDs. As our model input, we would like to take all possible image pairs within an ID. With 40 images per ID, this results in 780 pairs.\n",
    "\n",
    "Dataloader will output input image, input vector, target image, target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset folder\n",
    "def get_dataset(input_file_path, input_fn):\n",
    "    \"\"\"\n",
    "    Read the image and json data in the specified folder.\n",
    "    Args:\n",
    "        input_file_path: the base directory containing the dataset directories\n",
    "        input_fn: the name of dataset directory\n",
    "    Return:\n",
    "        imgs_list (list): List of images, cropped\n",
    "        vecs_list (list): List of gaze vectors, tiled\n",
    "    \"\"\"\n",
    "    img_infos = []\n",
    "    json_fns = glob(os.path.join(input_file_path, input_fn, \"*.json\"))\n",
    "    for json_fn in json_fns:\n",
    "        info = get_filename_info(json_fn)\n",
    "        img_infos.append(info)\n",
    "        \n",
    "    img_df = pd.DataFrame(img_infos)\n",
    "    img_df.sort_values(['ID', 'F'], ignore_index=True, inplace=True)\n",
    "    img_df.head()\n",
    "\n",
    "    # Extract relevant data for dataloader\n",
    "    n_ids = img_df.iloc[-1]['ID']\n",
    "    \n",
    "    imgs_list = []\n",
    "    vecs_list = []\n",
    "    \n",
    "    # For each ID, generate all possible pairs\n",
    "    for id in range(1, n_ids + 1):\n",
    "        # Get ID\n",
    "        df_chunk = img_df.query(f\"ID == {id}\")\n",
    "        imgs = np.stack(df_chunk['img'])\n",
    "        vecs = np.stack(df_chunk['look_vec'].to_numpy())\n",
    "        vecs = (vecs / np.linalg.norm(vecs, axis=1, keepdims=True))[:, :2] # normalize, then get x and y components\n",
    "        vecs = np.tile(vecs[:, :, np.newaxis, np.newaxis], (1, 1, 32, 64))\n",
    "    \n",
    "        # Generate pairs\n",
    "        pairs = np.triu_indices(len(df_chunk), k=1)\n",
    "        pairs = np.stack(pairs).transpose()\n",
    "    \n",
    "        img_pairs = imgs[pairs] # (pair_idx, input or output, C, H, W)\n",
    "        vec_pairs = vecs[pairs] # (pair_idx, input or output, x or y, ...)\n",
    "    \n",
    "        imgs_list.append(img_pairs)\n",
    "        vecs_list.append(vec_pairs)\n",
    "    \n",
    "    return imgs_list, vecs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                      | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading imgs_1_cutouts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████████████████▌                                                                                              | 1/4 [00:22<01:06, 22.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading imgs_2_cutouts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████████                                                               | 2/4 [00:47<00:47, 23.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading imgs_3_cutouts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████▌                               | 3/4 [01:05<00:21, 21.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading imgs_4_cutouts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:24<00:00, 21.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get all folders\n",
    "\n",
    "imgs_list_full = []\n",
    "vecs_list_full = []\n",
    "input_filename_list = ['imgs_1_cutouts', 'imgs_2_cutouts', 'imgs_3_cutouts', 'imgs_4_cutouts']\n",
    "input_file_path = os.path.join(os.getcwd(), '..', 'dataset', 'UnityEyes_Windows')\n",
    "\n",
    "print(\"Reading dataset\")\n",
    "for input_fn in tqdm(input_filename_list):\n",
    "    print(\"Reading \" + input_fn)\n",
    "    imgs_list_part, vecs_list_part = get_dataset(input_file_path, input_fn)\n",
    "    \n",
    "    imgs_list_full.append(imgs_list_part)\n",
    "    vecs_list_full.append(vecs_list_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312000, 2, 3, 32, 64)\n",
      "(312000, 2, 2, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "imgs_list_full = np.concatenate(np.concatenate(imgs_list_full))\n",
    "vecs_list_full = np.concatenate(np.concatenate(vecs_list_full))\n",
    "\n",
    "# Shuffle the lists. This takes a while\n",
    "np.random.shuffle(imgs_list_full)\n",
    "np.random.shuffle(vecs_list_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits\n",
    "num_samples = len(imgs_list_full)\n",
    "splits = [0.7, 0.2, 0.1]\n",
    "\n",
    "X_img = imgs_list_full[:, 0, ...]\n",
    "X_angle = vecs_list_full[:, 0, ...]\n",
    "y_img = imgs_list_full[:, 1, ...]\n",
    "y_angle = vecs_list_full[:, 1, ...]\n",
    "\n",
    "X_img_train, X_img_valid, X_img_test = np.split(X_img, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])\n",
    "X_angle_train, X_angle_valid, X_angle_test = np.split(X_angle, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])\n",
    "y_img_train, y_img_valid, y_img_test = np.split(y_img, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])\n",
    "y_angle_train, y_angle_valid, y_angle_test = np.split(y_angle, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Cb-oRNU8v8Ik"
   },
   "outputs": [],
   "source": [
    "X_img_train = torch.tensor(X_img_train, dtype=torch.int)\n",
    "X_angle_train = torch.tensor(X_angle_train, dtype=torch.float32)\n",
    "y_img_train = torch.tensor(y_img_train, dtype=torch.int)\n",
    "y_angle_train = torch.tensor(y_angle_train, dtype=torch.int)\n",
    "\n",
    "X_img_valid = torch.tensor(X_img_valid, dtype=torch.int)\n",
    "X_angle_valid = torch.tensor(X_angle_valid, dtype=torch.float32)\n",
    "y_img_valid = torch.tensor(y_img_valid, dtype=torch.int)\n",
    "y_angle_valid = torch.tensor(y_angle_valid, dtype=torch.int)\n",
    "\n",
    "X_img_test = torch.tensor(X_img_test, dtype=torch.int)\n",
    "X_angle_test = torch.tensor(X_angle_test, dtype=torch.float32)\n",
    "y_img_test = torch.tensor(y_img_test, dtype=torch.int)\n",
    "y_angle_test = torch.tensor(y_angle_test, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Uu2HyXd7wgn7"
   },
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "train_dataset = TensorDataset(X_img_train, X_angle_train, y_img_train, y_angle_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = TensorDataset(X_img_valid, X_angle_valid, y_img_valid, y_angle_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = TensorDataset(X_img_test, X_angle_test, y_img_test, y_angle_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                    | 0/100 [00:00<?, ?it/s]C:\\Users\\Codey\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\nn\\functional.py:4316: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "  0%|                                                                                                                                    | 0/100 [02:59<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     43\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "weight_correction_loss = 0.8\n",
    "weight_reconstruction_loss = 0.2\n",
    "\n",
    "printout_freq = 1\n",
    "start_time = time.time()\n",
    "\n",
    "warp = WarpImageWithFlowAndBrightness(next(iter(train_loader))[0])\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "pbar = tqdm(range(num_epochs))\n",
    "for epoch in pbar:\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for imgs, angles, targets, target_angles in train_loader:\n",
    "        imgs, angles, targets, target_angles = (\n",
    "            imgs.float().to(device), angles.float().to(device), targets.float().to(device), target_angles.float().to(device)\n",
    "        )\n",
    "        \n",
    "        flow_corr, bright_corr = model(imgs, target_angles)\n",
    "        img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "        loss_correction = criterion(img_corr, targets)\n",
    "        \n",
    "        flow_reconstruction, bright_reconstruction = model(img_corr, angles)\n",
    "        img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "        loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "        \n",
    "        loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss * loss_reconstruction)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        for imgs, angles, targets, target_angles in valid_loader:\n",
    "            imgs, angles, targets, target_angles = (\n",
    "                imgs.float().to(device), angles.float().to(device), targets.float().to(device), target_angles.float().to(device)\n",
    "            )\n",
    "\n",
    "            flow_corr, bright_corr = model(imgs, target_angles)\n",
    "            img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "            loss_correction = criterion(img_corr, targets)\n",
    "\n",
    "            flow_reconstruction, bright_reconstruction = model(img_corr, angles)\n",
    "            img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "            loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "\n",
    "            loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss * loss_reconstruction)\n",
    "            valid_loss += loss.item()\n",
    "        valid_losses.append(valid_loss / len(valid_loader))\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    overall_time = time.time() - start_time\n",
    "    num_days = int(overall_time / 86400)\n",
    "    num_hrs = int((overall_time-(86400*num_days)) / 3600)\n",
    "    num_mins = int((overall_time-(86400*num_days)-(3600*num_hrs)) / 60)\n",
    "    num_secs = overall_time-(86400*num_days)-(3600*num_hrs)*(60*num_mins)\n",
    "    if (epoch + 1) % printout_freq == 0:\n",
    "    #     print(f\"\"\"Finished epoch {epoch + 1}/{num_epochs} ({(epoch+1)/num_epochs*100:.2f}%)\n",
    "    #            total time is {num_days}d:{num_hrs}h:{num_mins}m:{num_secs:.3f}s; time for this epoch is {epoch_time:.2f}s\n",
    "    #            training loss was {bcolors.BOLD}{train_loss:.3f}{bcolors.ENDC}, validation_loss was {bcolors.BOLD}{valid_loss:.3f}{bcolors.ENDC}.\"\"\")\n",
    "        pbar.set_postfix({\"TLoss\": f\"{train_loss:.3f}\", \"VLoss\": f\"{valid_loss:.3f}\"})\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, angles, targets, target_angles in test_loader:\n",
    "        imgs, angles, targets, target_angles = (\n",
    "            imgs.float().to(device), angles.float().to(device), targets.float().to(device), target_angles.float().to(device)\n",
    "        )\n",
    "\n",
    "        flow_corr, bright_corr = model(imgs, target_angles)\n",
    "        img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "        loss_correction = criterion(img_corr, targets)\n",
    "        \n",
    "        flow_reconstruction, bright_reconstruction = model(img_corr, angles)\n",
    "        img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "        loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "        \n",
    "        loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss * loss_reconstruction)\n",
    "        test_loss += loss.item()\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
