{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWVAyg_GsMRN",
    "outputId": "fd891c81-410b-4020-df26-322a6eae0ba1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from ipdb import set_trace\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import model\n",
    "from warp import WarpImageWithFlowAndBrightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_list(json_list):\n",
    "    ldmks = [eval(s) for s in json_list]\n",
    "    return np.array([(x, y, z) for (x, y, z) in ldmks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_info(filename):\n",
    "    info_dict = {}\n",
    "    \n",
    "    # Get image data\n",
    "    img = cv2.cvtColor(cv2.imread(\"%s.jpg\" % filename[:-5]), cv2.COLOR_BGR2RGB)\n",
    "    # TODO: crop image\n",
    "    img = cv2.resize(img, (64, 32))\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    info_dict['img'] = img\n",
    "    \n",
    "\n",
    "    # Get ID data\n",
    "    titles_types = {'ID': int, 'T': str, 'N': int, 'F': int, 'V': float, 'H': float}\n",
    "    info_list = os.path.basename(filename[:-5]).split('_')\n",
    "    for title, info in zip(titles_types, info_list):\n",
    "        info_dict[title] = titles_types[title](info[len(title):])\n",
    "    info_dict['target'] = info_dict['F'] == 1\n",
    "        \n",
    "    # Get json data\n",
    "    json_data_file = open(filename)\n",
    "    json_data = json.load(json_data_file)\n",
    "    info_dict[\"look_vec\"] = np.array(eval(json_data[\"eye_details\"][\"look_vec\"])) # 3D look direction vector\n",
    "    info_dict[\"interior_margin\"] = process_json_list(json_data[\"interior_margin_2d\"]) # eye interior landmarks\n",
    "    info_dict[\"caruncle\"] = process_json_list(json_data[\"caruncle_2d\"]) # caruncle landmarks\n",
    "    info_dict[\"iris\"] = process_json_list(json_data[\"iris_2d\"]) # iris landmarks\n",
    "    \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HvMcnkjhsU3B",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_infos = []\n",
    "input_file_path = os.path.join(os.getcwd(), '..', 'dataset', 'UnityEyes_Windows', 'imgs_1_cutouts')\n",
    "\n",
    "json_fns = glob(os.path.join(input_file_path, \"*.json\"))\n",
    "for json_fn in json_fns:\n",
    "    info = get_filename_info(json_fn)\n",
    "    img_infos.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>ID</th>\n",
       "      <th>T</th>\n",
       "      <th>N</th>\n",
       "      <th>F</th>\n",
       "      <th>V</th>\n",
       "      <th>H</th>\n",
       "      <th>target</th>\n",
       "      <th>look_vec</th>\n",
       "      <th>interior_margin</th>\n",
       "      <th>caruncle</th>\n",
       "      <th>iris</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...</td>\n",
       "      <td>1</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.0016, 0.0, -1.0, 0.0]</td>\n",
       "      <td>[[352.8355, 265.1642, 9.0855], [349.2318, 278....</td>\n",
       "      <td>[[340.2054, 248.9607, 9.096], [333.4014, 259.0...</td>\n",
       "      <td>[[356.5728, 299.975, 8.9238], [357.405, 308.47...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...</td>\n",
       "      <td>1</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.13</td>\n",
       "      <td>4.80</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0832, -0.1068, -0.9908, 0.0]</td>\n",
       "      <td>[[352.9997, 264.6737, 9.0874], [349.6505, 277....</td>\n",
       "      <td>[[340.1695, 248.8329, 9.0979], [333.4014, 259....</td>\n",
       "      <td>[[363.3105, 290.1617, 8.8879], [363.9812, 298....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...</td>\n",
       "      <td>1</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.77</td>\n",
       "      <td>10.84</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.1874, 0.0831, -0.9788, 0.0]</td>\n",
       "      <td>[[352.6367, 265.7174, 9.0836], [348.7342, 279....</td>\n",
       "      <td>[[340.2437, 249.1077, 9.094], [333.4014, 259.0...</td>\n",
       "      <td>[[372.4717, 303.7707, 8.8355], [372.5953, 312....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...</td>\n",
       "      <td>1</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-13.43</td>\n",
       "      <td>-5.19</td>\n",
       "      <td>False</td>\n",
       "      <td>[-0.088, -0.2322, -0.9687, 0.0]</td>\n",
       "      <td>[[353.3353, 263.5461, 9.0922], [350.5352, 276....</td>\n",
       "      <td>[[340.0798, 248.548, 9.1021], [333.4014, 259.0...</td>\n",
       "      <td>[[349.5125, 282.5456, 9.007], [350.4209, 290.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...</td>\n",
       "      <td>1</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-10.65</td>\n",
       "      <td>9.39</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.1603, -0.1848, -0.9696, 0.0]</td>\n",
       "      <td>[[353.2023, 264.0168, 9.0901], [350.1789, 276....</td>\n",
       "      <td>[[340.1184, 248.6655, 9.1004], [333.4014, 259....</td>\n",
       "      <td>[[369.9451, 282.7769, 8.8686], [370.5723, 291....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 img  ID         T  N  F  \\\n",
       "0  [[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...   1  gaussian  1  1   \n",
       "1  [[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...   1  gaussian  1  2   \n",
       "2  [[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...   1  gaussian  1  3   \n",
       "3  [[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...   1  gaussian  1  4   \n",
       "4  [[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...   1  gaussian  1  5   \n",
       "\n",
       "       V      H  target                         look_vec  \\\n",
       "0   0.00   0.09    True         [0.0016, 0.0, -1.0, 0.0]   \n",
       "1  -6.13   4.80   False  [0.0832, -0.1068, -0.9908, 0.0]   \n",
       "2   4.77  10.84   False   [0.1874, 0.0831, -0.9788, 0.0]   \n",
       "3 -13.43  -5.19   False  [-0.088, -0.2322, -0.9687, 0.0]   \n",
       "4 -10.65   9.39   False  [0.1603, -0.1848, -0.9696, 0.0]   \n",
       "\n",
       "                                     interior_margin  \\\n",
       "0  [[352.8355, 265.1642, 9.0855], [349.2318, 278....   \n",
       "1  [[352.9997, 264.6737, 9.0874], [349.6505, 277....   \n",
       "2  [[352.6367, 265.7174, 9.0836], [348.7342, 279....   \n",
       "3  [[353.3353, 263.5461, 9.0922], [350.5352, 276....   \n",
       "4  [[353.2023, 264.0168, 9.0901], [350.1789, 276....   \n",
       "\n",
       "                                            caruncle  \\\n",
       "0  [[340.2054, 248.9607, 9.096], [333.4014, 259.0...   \n",
       "1  [[340.1695, 248.8329, 9.0979], [333.4014, 259....   \n",
       "2  [[340.2437, 249.1077, 9.094], [333.4014, 259.0...   \n",
       "3  [[340.0798, 248.548, 9.1021], [333.4014, 259.0...   \n",
       "4  [[340.1184, 248.6655, 9.1004], [333.4014, 259....   \n",
       "\n",
       "                                                iris  \n",
       "0  [[356.5728, 299.975, 8.9238], [357.405, 308.47...  \n",
       "1  [[363.3105, 290.1617, 8.8879], [363.9812, 298....  \n",
       "2  [[372.4717, 303.7707, 8.8355], [372.5953, 312....  \n",
       "3  [[349.5125, 282.5456, 9.007], [350.4209, 290.7...  \n",
       "4  [[369.9451, 282.7769, 8.8686], [370.5723, 291....  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_df = pd.DataFrame(img_infos)\n",
    "img_df.sort_values(['ID', 'F'], ignore_index=True, inplace=True)\n",
    "img_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the dataframe info\n",
    "- look_vec is a 3D homogeneous vector in the form \\[x, y, z, 0\\]. For purposes of the model input, only the x and y components are needed. I believe this vector is already normalized, but we can renormalize this vector before extracting the x and y components\n",
    "- Feature landmarks (i.e., interior_margin, caruncle, and iris) are the pixel coordinates of feature BEFORE RESIZING. Ideally they are used to determine the crop/resize area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "The above dataframe contains info on all images, separated into IDs. As our model input, we would like to take all possible image pairs within an ID. With 40 images per ID, this results in 780 pairs.\n",
    "\n",
    "Dataloader will output input image, input vector, target image, target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant data for dataloader\n",
    "n_ids = img_df.iloc[-1]['ID']\n",
    "\n",
    "imgs_list = []\n",
    "vecs_list = []\n",
    "\n",
    "# For each ID, generate all possible pairs\n",
    "for id in range(1, n_ids + 1):\n",
    "    # Get ID\n",
    "    df_chunk = img_df.query(f\"ID == {id}\")\n",
    "    imgs = np.stack(df_chunk['img'])\n",
    "    vecs = np.stack(df_chunk['look_vec'].to_numpy())\n",
    "    vecs = (vecs / np.linalg.norm(vecs, axis=1, keepdims=True))[:, :2] # normalize, then get x and y components\n",
    "    vecs = np.tile(vecs[:, :, np.newaxis, np.newaxis], (1, 1, 32, 64))\n",
    "\n",
    "    # Generate pairs\n",
    "    pairs = np.triu_indices(len(df_chunk), k=1)\n",
    "    pairs = np.stack(pairs).transpose()\n",
    "\n",
    "    img_pairs = imgs[pairs] # (pair_idx, input or output, C, H, W)\n",
    "    vec_pairs = vecs[pairs] # (pair_idx, input or output, x or y, ...)\n",
    "\n",
    "    imgs_list.append(img_pairs)\n",
    "    vecs_list.append(vec_pairs)\n",
    "\n",
    "imgs_list = np.concatenate(imgs_list)\n",
    "vecs_list = np.concatenate(vecs_list)\n",
    "\n",
    "# Shuffle the lists\n",
    "np.random.shuffle(imgs_list)\n",
    "np.random.shuffle(vecs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits\n",
    "num_samples = len(imgs_list)\n",
    "splits = [0.7, 0.2, 0.1]\n",
    "\n",
    "X_img = imgs_list[:, 0, ...]\n",
    "X_angle = vecs_list[:, 0, ...]\n",
    "y_img = imgs_list[:, 1, ...]\n",
    "y_angle = vecs_list[:, 1, ...]\n",
    "\n",
    "X_img_train, X_img_valid, X_img_test = np.split(X_img, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])\n",
    "X_angle_train, X_angle_valid, X_angle_test = np.split(X_angle, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])\n",
    "y_img_train, y_img_valid, y_img_test = np.split(y_img, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])\n",
    "y_angle_train, y_angle_valid, y_angle_test = np.split(y_angle, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "Cb-oRNU8v8Ik"
   },
   "outputs": [],
   "source": [
    "X_img_train = torch.tensor(X_img_train, dtype=torch.int)\n",
    "X_angle_train = torch.tensor(X_angle_train, dtype=torch.float32)\n",
    "y_img_train = torch.tensor(y_img_train, dtype=torch.int)\n",
    "y_angle_train = torch.tensor(y_angle_train, dtype=torch.int)\n",
    "\n",
    "X_img_valid = torch.tensor(X_img_valid, dtype=torch.int)\n",
    "X_angle_valid = torch.tensor(X_angle_valid, dtype=torch.float32)\n",
    "y_img_valid = torch.tensor(y_img_valid, dtype=torch.int)\n",
    "y_angle_valid = torch.tensor(y_angle_valid, dtype=torch.int)\n",
    "\n",
    "X_img_test = torch.tensor(X_img_test, dtype=torch.int)\n",
    "X_angle_test = torch.tensor(X_angle_test, dtype=torch.float32)\n",
    "y_img_test = torch.tensor(y_img_test, dtype=torch.int)\n",
    "y_angle_test = torch.tensor(y_angle_test, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "Uu2HyXd7wgn7"
   },
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "train_dataset = TensorDataset(X_img_train, X_angle_train, y_img_train, y_angle_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = TensorDataset(X_img_valid, X_angle_valid, y_img_valid, y_angle_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = TensorDataset(X_img_test, X_angle_test, y_img_test, y_angle_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 0/100 [00:00<?, ?it/s]C:\\Users\\Codey\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\nn\\functional.py:4316: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "  0%|                                                                           | 0/100 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     39\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 40\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     42\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "weight_correction_loss = 0.8\n",
    "weight_reconstruction_loss = 0.2\n",
    "\n",
    "printout_freq = 1\n",
    "start_time = time.time()\n",
    "\n",
    "warp = WarpImageWithFlowAndBrightness(next(iter(train_loader))[0])\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "pbar = tqdm(range(num_epochs))\n",
    "for epoch in pbar:\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for imgs, angles, targets, target_angles in train_loader:\n",
    "        imgs, angles, targets = imgs.float().to(device), angles.to(device), targets.float().to(device)\n",
    "        \n",
    "        flow_corr, bright_corr = model(imgs, angles)\n",
    "        img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "        loss_correction = criterion(img_corr, targets)\n",
    "        \n",
    "        inverted_angles = -angles\n",
    "        flow_reconstruction, bright_reconstruction = model(img_corr, inverted_angles)\n",
    "        img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "        loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "        \n",
    "        loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss) * loss_reconstruction\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        for imgs, angles, targets, target_angles in valid_loader:\n",
    "            imgs, angles, targets = imgs.float().to(device), angles.to(device), targets.float().to(device)\n",
    "\n",
    "            flow_corr, bright_corr = model(imgs, angles)\n",
    "            img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "            loss_correction = criterion(img_corr, targets)\n",
    "\n",
    "            inverted_angles = -angles\n",
    "            flow_reconstruction, bright_reconstruction = model(img_corr, inverted_angles)\n",
    "            img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "            loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "\n",
    "            loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss) * loss_reconstruction\n",
    "            valid_loss += loss.item()\n",
    "        valid_losses.append(valid_loss / len(valid_loader))\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    overall_time = time.time() - start_time\n",
    "    num_days = int(overall_time / 86400)\n",
    "    num_hrs = int((overall_time-(86400*num_days)) / 3600)\n",
    "    num_mins = int((overall_time-(86400*num_days)-(3600*num_hrs)) / 60)\n",
    "    num_secs = overall_time-(86400*num_days)-(3600*num_hrs)*(60*num_mins)\n",
    "    if (epoch + 1) % printout_freq == 0:\n",
    "    #     print(f\"\"\"Finished epoch {epoch + 1}/{num_epochs} ({(epoch+1)/num_epochs*100:.2f}%)\n",
    "    #            total time is {num_days}d:{num_hrs}h:{num_mins}m:{num_secs:.3f}s; time for this epoch is {epoch_time:.2f}s\n",
    "    #            training loss was {bcolors.BOLD}{train_loss:.3f}{bcolors.ENDC}, validation_loss was {bcolors.BOLD}{valid_loss:.3f}{bcolors.ENDC}.\"\"\")\n",
    "        pbar.set_postfix({\"TLoss\": f\"{train_loss:.3f}\", \"VLoss\": f\"{valid_loss:.3f}\"})\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, angles, targets, target_angles in test_loader:\n",
    "        imgs, angles, targets = imgs.float().to(device), angles.to(device), targets.float().to(device)\n",
    "\n",
    "        flow_corr, bright_corr = model(imgs, angles)\n",
    "        img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "        loss_correction = criterion(img_corr, targets)\n",
    "        \n",
    "        inverted_angles = -angles\n",
    "        flow_reconstruction, bright_reconstruction = model(img_corr, inverted_angles)\n",
    "        img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "        loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "        \n",
    "        loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss) * loss_reconstruction\n",
    "        test_loss += loss.item()\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
