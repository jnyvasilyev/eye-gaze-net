{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWVAyg_GsMRN",
    "outputId": "fd891c81-410b-4020-df26-322a6eae0ba1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from ipdb import set_trace\n",
    "from glob import glob\n",
    "\n",
    "from model import model\n",
    "from warp import WarpImageWithFlowAndBrightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_list(json_list):\n",
    "    ldmks = [eval(s) for s in json_list]\n",
    "    return np.array([(x, y, z) for (x, y, z) in ldmks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_info(filename):\n",
    "    info_dict = {}\n",
    "    \n",
    "    # Get image data\n",
    "    img = cv2.cvtColor(cv2.imread(\"%s.jpg\" % filename[:-5]), cv2.COLOR_BGR2RGB)\n",
    "    # TODO: crop image\n",
    "    img = cv2.resize(img, (64, 32))\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    info_dict['img'] = img\n",
    "    \n",
    "\n",
    "    # Get ID data\n",
    "    titles_types = {'ID': int, 'T': str, 'N': int, 'F': int, 'V': float, 'H': float}\n",
    "    info_list = os.path.basename(filename[:-5]).split('_')\n",
    "    for title, info in zip(titles_types, info_list):\n",
    "        info_dict[title] = titles_types[title](info[len(title):])\n",
    "    info_dict['target'] = info_dict['F'] == 1\n",
    "        \n",
    "    # Get json data\n",
    "    json_data_file = open(filename)\n",
    "    json_data = json.load(json_data_file)\n",
    "    info_dict[\"look_vec\"] = np.array(eval(json_data[\"eye_details\"][\"look_vec\"])) # 3D look direction vector\n",
    "    info_dict[\"interior_margin\"] = process_json_list(json_data[\"interior_margin_2d\"]) # eye interior landmarks\n",
    "    info_dict[\"caruncle\"] = process_json_list(json_data[\"caruncle_2d\"]) # caruncle landmarks\n",
    "    info_dict[\"iris\"] = process_json_list(json_data[\"iris_2d\"]) # iris landmarks\n",
    "    \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HvMcnkjhsU3B",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_infos = []\n",
    "input_file_path = os.path.join(os.getcwd(), '..', 'dataset', 'UnityEyes_Windows', 'imgs_1_cutouts')\n",
    "\n",
    "json_fns = glob(os.path.join(input_file_path, \"*.json\"))\n",
    "for json_fn in json_fns:\n",
    "    info = get_filename_info(json_fn)\n",
    "    img_infos.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>ID</th>\n",
       "      <th>T</th>\n",
       "      <th>N</th>\n",
       "      <th>F</th>\n",
       "      <th>V</th>\n",
       "      <th>H</th>\n",
       "      <th>target</th>\n",
       "      <th>look_vec</th>\n",
       "      <th>interior_margin</th>\n",
       "      <th>caruncle</th>\n",
       "      <th>iris</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...</td>\n",
       "      <td>1</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.0016, 0.0, -1.0, 0.0]</td>\n",
       "      <td>[[352.8355, 265.1642, 9.0855], [349.2318, 278....</td>\n",
       "      <td>[[340.2054, 248.9607, 9.096], [333.4014, 259.0...</td>\n",
       "      <td>[[356.5728, 299.975, 8.9238], [357.405, 308.47...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...</td>\n",
       "      <td>1</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.13</td>\n",
       "      <td>4.80</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.0832, -0.1068, -0.9908, 0.0]</td>\n",
       "      <td>[[352.9997, 264.6737, 9.0874], [349.6505, 277....</td>\n",
       "      <td>[[340.1695, 248.8329, 9.0979], [333.4014, 259....</td>\n",
       "      <td>[[363.3105, 290.1617, 8.8879], [363.9812, 298....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...</td>\n",
       "      <td>1</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.77</td>\n",
       "      <td>10.84</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.1874, 0.0831, -0.9788, 0.0]</td>\n",
       "      <td>[[352.6367, 265.7174, 9.0836], [348.7342, 279....</td>\n",
       "      <td>[[340.2437, 249.1077, 9.094], [333.4014, 259.0...</td>\n",
       "      <td>[[372.4717, 303.7707, 8.8355], [372.5953, 312....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...</td>\n",
       "      <td>1</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-13.43</td>\n",
       "      <td>-5.19</td>\n",
       "      <td>False</td>\n",
       "      <td>[-0.088, -0.2322, -0.9687, 0.0]</td>\n",
       "      <td>[[353.3353, 263.5461, 9.0922], [350.5352, 276....</td>\n",
       "      <td>[[340.0798, 248.548, 9.1021], [333.4014, 259.0...</td>\n",
       "      <td>[[349.5125, 282.5456, 9.007], [350.4209, 290.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...</td>\n",
       "      <td>1</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-10.65</td>\n",
       "      <td>9.39</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.1603, -0.1848, -0.9696, 0.0]</td>\n",
       "      <td>[[353.2023, 264.0168, 9.0901], [350.1789, 276....</td>\n",
       "      <td>[[340.1184, 248.6655, 9.1004], [333.4014, 259....</td>\n",
       "      <td>[[369.9451, 282.7769, 8.8686], [370.5723, 291....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 img  ID         T  N  F  \\\n",
       "0  [[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...   1  gaussian  1  1   \n",
       "1  [[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...   1  gaussian  1  2   \n",
       "2  [[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...   1  gaussian  1  3   \n",
       "3  [[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...   1  gaussian  1  4   \n",
       "4  [[[97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,...   1  gaussian  1  5   \n",
       "\n",
       "       V      H  target                         look_vec  \\\n",
       "0   0.00   0.09    True         [0.0016, 0.0, -1.0, 0.0]   \n",
       "1  -6.13   4.80   False  [0.0832, -0.1068, -0.9908, 0.0]   \n",
       "2   4.77  10.84   False   [0.1874, 0.0831, -0.9788, 0.0]   \n",
       "3 -13.43  -5.19   False  [-0.088, -0.2322, -0.9687, 0.0]   \n",
       "4 -10.65   9.39   False  [0.1603, -0.1848, -0.9696, 0.0]   \n",
       "\n",
       "                                     interior_margin  \\\n",
       "0  [[352.8355, 265.1642, 9.0855], [349.2318, 278....   \n",
       "1  [[352.9997, 264.6737, 9.0874], [349.6505, 277....   \n",
       "2  [[352.6367, 265.7174, 9.0836], [348.7342, 279....   \n",
       "3  [[353.3353, 263.5461, 9.0922], [350.5352, 276....   \n",
       "4  [[353.2023, 264.0168, 9.0901], [350.1789, 276....   \n",
       "\n",
       "                                            caruncle  \\\n",
       "0  [[340.2054, 248.9607, 9.096], [333.4014, 259.0...   \n",
       "1  [[340.1695, 248.8329, 9.0979], [333.4014, 259....   \n",
       "2  [[340.2437, 249.1077, 9.094], [333.4014, 259.0...   \n",
       "3  [[340.0798, 248.548, 9.1021], [333.4014, 259.0...   \n",
       "4  [[340.1184, 248.6655, 9.1004], [333.4014, 259....   \n",
       "\n",
       "                                                iris  \n",
       "0  [[356.5728, 299.975, 8.9238], [357.405, 308.47...  \n",
       "1  [[363.3105, 290.1617, 8.8879], [363.9812, 298....  \n",
       "2  [[372.4717, 303.7707, 8.8355], [372.5953, 312....  \n",
       "3  [[349.5125, 282.5456, 9.007], [350.4209, 290.7...  \n",
       "4  [[369.9451, 282.7769, 8.8686], [370.5723, 291....  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_df = pd.DataFrame(img_infos)\n",
    "img_df.sort_values(['ID', 'F'], ignore_index=True, inplace=True)\n",
    "img_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the dataframe info\n",
    "- look_vec is a 3D homogeneous vector in the form \\[x, y, z, 0\\]. For purposes of the model input, only the x and y components are needed. I believe this vector is already normalized, but we can renormalize this vector before extracting the x and y components\n",
    "- Feature landmarks (i.e., interior_margin, caruncle, and iris) are the pixel coordinates of feature BEFORE RESIZING. Ideally they are used to determine the crop/resize area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img = np.stack(img_df.query('target == False')['img'])\n",
    "# X_angle = img_df.query('target == False')[['H', 'V']].to_numpy()\n",
    "# Get x and y components of look_vec\n",
    "X_angle = np.stack(img_df.query('target == False')['look_vec'].to_numpy())[:, :2]\n",
    "X_angle = np.tile(X_angle[:, :, np.newaxis, np.newaxis], (1, 1, 32, 64))\n",
    "y = np.stack(img_df.query('target == True')['img'])\n",
    "y = np.repeat(y, 39, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "fmonNWBAs_Dk"
   },
   "outputs": [],
   "source": [
    "num_samples = X_img.shape[0]\n",
    "splits = [0.7, 0.2, 0.1]\n",
    "\n",
    "X_img_train, X_img_valid, X_img_test = np.split(X_img, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])\n",
    "X_angle_train, X_angle_valid, X_angle_test = np.split(X_angle, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])\n",
    "y_train, y_valid, y_test = np.split(y, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Cb-oRNU8v8Ik"
   },
   "outputs": [],
   "source": [
    "X_img_train = torch.tensor(X_img_train, dtype=torch.int)\n",
    "X_angle_train = torch.tensor(X_angle_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.int)\n",
    "\n",
    "X_img_valid = torch.tensor(X_img_valid, dtype=torch.int)\n",
    "X_angle_valid = torch.tensor(X_angle_valid, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.int)\n",
    "\n",
    "X_img_test = torch.tensor(X_img_test, dtype=torch.int)\n",
    "X_angle_test = torch.tensor(X_angle_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Uu2HyXd7wgn7"
   },
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "train_dataset = TensorDataset(X_img_train, X_angle_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = TensorDataset(X_img_valid, X_angle_valid, y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = TensorDataset(X_img_test, X_angle_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Codey\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\nn\\functional.py:4316: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1/100 (1.00%)\n",
      "               total time is 0d:0h:0m:31.635s; time for this epoch is 31.63s\n",
      "               training loss was \u001b[1m221424.078\u001b[0m, validation_loss was \u001b[1m51633.316\u001b[0m.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m loss_correction \u001b[38;5;241m=\u001b[39m criterion(img_corr, targets)\n\u001b[0;32m     29\u001b[0m inverted_angles \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mangles\n\u001b[1;32m---> 30\u001b[0m flow_reconstruction, bright_reconstruction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_corr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverted_angles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m img_reconstruction \u001b[38;5;241m=\u001b[39m warp(img_corr, flow_reconstruction, bright_reconstruction)\n\u001b[0;32m     32\u001b[0m loss_reconstruction \u001b[38;5;241m=\u001b[39m criterion(img_reconstruction, imgs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\@School\\Spring2024\\SeniorDesign\\eye-gaze-net\\model.py:60\u001b[0m, in \u001b[0;36mECCNet.forward\u001b[1;34m(self, img, angle)\u001b[0m\n\u001b[0;32m     58\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block3(x3)\n\u001b[0;32m     59\u001b[0m x4 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x3, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_block4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Decoder with skip connections\u001b[39;00m\n\u001b[0;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(x4, scale_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\@School\\Spring2024\\SeniorDesign\\eye-gaze-net\\model.py:30\u001b[0m, in \u001b[0;36mResidualConvBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Residual skip connection, might need to add downsample depending on input and output channels\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m---> 30\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39madd(out, residual)\n\u001b[0;32m     32\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\@School\\Spring2024\\SeniorDesign\\eye-gaze-net\\model.py:16\u001b[0m, in \u001b[0;36mDepthwiseSeparableConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepthwise(x)\n\u001b[0;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpointwise(x)\n\u001b[1;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\nn\\functional.py:2482\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2480\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2484\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "weight_correction_loss = 0.8\n",
    "weight_reconstruction_loss = 0.2\n",
    "\n",
    "printout_freq = 1\n",
    "start_time = time.time()\n",
    "\n",
    "warp = WarpImageWithFlowAndBrightness(next(iter(train_loader))[0])\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for imgs, angles, targets in train_loader:\n",
    "        imgs, angles, targets = imgs.float().to(device), angles.to(device), targets.float().to(device)\n",
    "        \n",
    "        flow_corr, bright_corr = model(imgs, angles)\n",
    "        img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "        loss_correction = criterion(img_corr, targets)\n",
    "        \n",
    "        inverted_angles = -angles\n",
    "        flow_reconstruction, bright_reconstruction = model(img_corr, inverted_angles)\n",
    "        img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "        loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "        \n",
    "        loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss) * loss_reconstruction\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        for imgs, angles, targets in valid_loader:\n",
    "            imgs, angles, targets = imgs.float().to(device), angles.to(device), targets.float().to(device)\n",
    "\n",
    "            flow_corr, bright_corr = model(imgs, angles)\n",
    "            img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "            loss_correction = criterion(img_corr, targets)\n",
    "\n",
    "            inverted_angles = -angles\n",
    "            flow_reconstruction, bright_reconstruction = model(img_corr, inverted_angles)\n",
    "            img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "            loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "\n",
    "            loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss) * loss_reconstruction\n",
    "            valid_loss += loss.item()\n",
    "        valid_losses.append(valid_loss / len(valid_loader))\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    overall_time = time.time() - start_time\n",
    "    num_days = int(overall_time / 86400)\n",
    "    num_hrs = int((overall_time-(86400*num_days)) / 3600)\n",
    "    num_mins = int((overall_time-(86400*num_days)-(3600*num_hrs)) / 60)\n",
    "    num_secs = overall_time-(86400*num_days)-(3600*num_hrs)*(60*num_mins)\n",
    "    if (epoch + 1) % printout_freq == 0:\n",
    "        print(f\"\"\"Finished epoch {epoch + 1}/{num_epochs} ({(epoch+1)/num_epochs*100:.2f}%)\n",
    "               total time is {num_days}d:{num_hrs}h:{num_mins}m:{num_secs:.3f}s; time for this epoch is {epoch_time:.2f}s\n",
    "               training loss was {bcolors.BOLD}{train_loss:.3f}{bcolors.ENDC}, validation_loss was {bcolors.BOLD}{valid_loss:.3f}{bcolors.ENDC}.\"\"\")\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, angles, targets in test_loader:\n",
    "        imgs, angles, targets = imgs.float().to(device), angles.to(device), targets.float().to(device)\n",
    "\n",
    "        flow_corr, bright_corr = model(imgs, angles)\n",
    "        img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "        loss_correction = criterion(img_corr, targets)\n",
    "        \n",
    "        inverted_angles = -angles\n",
    "        flow_reconstruction, bright_reconstruction = model(img_corr, inverted_angles)\n",
    "        img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "        loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "        \n",
    "        loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss) * loss_reconstruction\n",
    "        test_loss += loss.item()\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
