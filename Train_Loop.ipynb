{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWVAyg_GsMRN",
    "outputId": "fd891c81-410b-4020-df26-322a6eae0ba1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from ipdb import set_trace\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import model\n",
    "from warp import WarpImageWithFlowAndBrightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_list(json_list):\n",
    "    ldmks = [eval(s) for s in json_list]\n",
    "    return np.array([(x, y, z) for (x, y, z) in ldmks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_info(filename):\n",
    "    info_dict = {}\n",
    "\n",
    "    # Get ID data\n",
    "    titles_types = {'ID': int, 'T': str, 'N': int, 'F': int, 'V': float, 'H': float}\n",
    "    info_list = os.path.basename(filename[:-5]).split('_')\n",
    "    for title, info in zip(titles_types, info_list):\n",
    "        info_dict[title] = titles_types[title](info[len(title):])\n",
    "    info_dict['target'] = info_dict['F'] == 1\n",
    "    \n",
    "    # Get json data\n",
    "    json_data_file = open(filename)\n",
    "    json_data = json.load(json_data_file)\n",
    "    info_dict[\"look_vec\"] = np.array(eval(json_data[\"eye_details\"][\"look_vec\"])) # 3D look direction vector\n",
    "    info_dict[\"interior_margin\"] = process_json_list(json_data[\"interior_margin_2d\"]) # eye interior landmarks\n",
    "    info_dict[\"caruncle\"] = process_json_list(json_data[\"caruncle_2d\"]) # caruncle landmarks\n",
    "    info_dict[\"iris\"] = process_json_list(json_data[\"iris_2d\"]) # iris landmarks\n",
    "    json_data_file.close()\n",
    "    \n",
    "    # Get image data\n",
    "    img = cv2.cvtColor(cv2.imread(\"%s.jpg\" % filename[:-5]), cv2.COLOR_BGR2RGB)\n",
    "    eye_hor_range = np.max(info_dict['interior_margin'], axis=0)[0] - np.min(info_dict['interior_margin'], axis=0)[0]\n",
    "    eye_vert_range = np.max(info_dict['interior_margin'], axis=0)[1] - np.min(info_dict['interior_margin'], axis=0)[1]\n",
    "    if eye_hor_range // 2 > eye_vert_range:\n",
    "        eye_vert_range = eye_hor_range // 2\n",
    "    else:\n",
    "        eye_hor_range = eye_vert_range * 2\n",
    "    eyeball_center = np.array([300, 400])\n",
    "    range_coef = 2\n",
    "    img_cropped = img[eyeball_center[0] - int(eye_vert_range // 2 * range_coef) : eyeball_center[0] + int(eye_vert_range // 2 * range_coef),\n",
    "                      eyeball_center[1] - int(eye_hor_range // 2 * range_coef) : eyeball_center[1] + int(eye_hor_range // 2 * range_coef)]\n",
    "    img = cv2.resize(img_cropped, (64, 32))\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    info_dict['img'] = img\n",
    "    \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = '../dataset/UnityEyes_Windows/imgs_2_cutouts/ID1_Tgaussian_N001_F1_V0.00_H0.02.json'\n",
    "info_dict = get_filename_info(test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the dataframe info\n",
    "- look_vec is a 3D homogeneous vector in the form \\[x, y, z, 0\\]. For purposes of the model input, only the x and y components are needed. I believe this vector is already normalized, but we can renormalize this vector before extracting the x and y components\n",
    "- Feature landmarks (i.e., interior_margin, caruncle, and iris) are the pixel coordinates of feature BEFORE RESIZING. Ideally they are used to determine the crop/resize area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "The above dataframe contains info on all images, separated into IDs. As our model input, we would like to take all possible image pairs within an ID. With 40 images per ID, this results in 780 pairs.\n",
    "\n",
    "Dataloader will output input image, input vector, target image, target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset folder\n",
    "def get_dataset(input_file_path, input_fn):\n",
    "    \"\"\"\n",
    "    Read the image and json data in the specified folder.\n",
    "    Args:\n",
    "        input_file_path: the base directory containing the dataset directories\n",
    "        input_fn: the name of dataset directory\n",
    "    Return:\n",
    "        imgs_list (list): List of images, cropped\n",
    "        vecs_list (list): List of gaze vectors, tiled\n",
    "    \"\"\"\n",
    "    img_infos = []\n",
    "    json_fns = glob(os.path.join(input_file_path, input_fn, \"*.json\"))\n",
    "    for json_fn in json_fns:\n",
    "        info = get_filename_info(json_fn)\n",
    "        img_infos.append(info)\n",
    "        \n",
    "    img_df = pd.DataFrame(img_infos)\n",
    "    img_df.sort_values(['ID', 'F'], ignore_index=True, inplace=True)\n",
    "\n",
    "    # Extract relevant data for dataloader\n",
    "    n_ids = img_df.iloc[-1]['ID']\n",
    "    \n",
    "    imgs_list = []\n",
    "    vecs_list = []\n",
    "    \n",
    "    # For each ID, generate all possible pairs\n",
    "    for id in range(1, n_ids + 1):\n",
    "        # Get ID\n",
    "        df_chunk = img_df.query(f\"ID == {id}\")\n",
    "        imgs = np.stack(df_chunk['img'])\n",
    "        vecs = np.stack(df_chunk['look_vec'].to_numpy())\n",
    "        vecs = (vecs / np.linalg.norm(vecs, axis=1, keepdims=True))[:, :2] # normalize, then get x and y components\n",
    "        vecs = np.tile(vecs[:, :, np.newaxis, np.newaxis], (1, 1, 32, 64))\n",
    "    \n",
    "        # Generate pairs\n",
    "        pairs = np.triu_indices(len(df_chunk), k=1)\n",
    "        pairs = np.stack(pairs).transpose()\n",
    "    \n",
    "        img_pairs = imgs[pairs] # (pair_idx, input or output, C, H, W)\n",
    "        vec_pairs = vecs[pairs] # (pair_idx, input or output, x or y, ...)\n",
    "    \n",
    "        imgs_list.append(img_pairs)\n",
    "        vecs_list.append(vec_pairs)\n",
    "    \n",
    "    return imgs_list, vecs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                      | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading imgs_0_cutouts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_fn \u001b[38;5;129;01min\u001b[39;00m tqdm(input_filename_list):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m input_fn)\n\u001b[1;32m---> 12\u001b[0m     imgs_list_part, vecs_list_part \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     imgs_list_full\u001b[38;5;241m.\u001b[39mappend(imgs_list_part)\n\u001b[0;32m     15\u001b[0m     vecs_list_full\u001b[38;5;241m.\u001b[39mappend(vecs_list_part)\n",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m, in \u001b[0;36mget_dataset\u001b[1;34m(input_file_path, input_fn)\u001b[0m\n\u001b[0;32m     13\u001b[0m json_fns \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_file_path, input_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m json_fn \u001b[38;5;129;01min\u001b[39;00m json_fns:\n\u001b[1;32m---> 15\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[43mget_filename_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     img_infos\u001b[38;5;241m.\u001b[39mappend(info)\n\u001b[0;32m     18\u001b[0m img_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(img_infos)\n",
      "Cell \u001b[1;32mIn[16], line 21\u001b[0m, in \u001b[0;36mget_filename_info\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     18\u001b[0m json_data_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Get image data\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m eye_hor_range \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(info_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterior_margin\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(info_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterior_margin\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     23\u001b[0m eye_vert_range \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(info_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterior_margin\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(info_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterior_margin\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# Get all folders\n",
    "\n",
    "imgs_list_full = []\n",
    "vecs_list_full = []\n",
    "input_filename_list = ['imgs_1_cutouts', 'imgs_2_cutouts', 'imgs_3_cutouts', 'imgs_4_cutouts']\n",
    "input_file_path = os.path.join(os.getcwd(), '..', 'dataset', 'UnityEyes_Windows')\n",
    "\n",
    "print(\"Reading dataset\")\n",
    "for input_fn in tqdm(input_filename_list):\n",
    "    print(\"Reading \" + input_fn)\n",
    "    imgs_list_part, vecs_list_part = get_dataset(input_file_path, input_fn)\n",
    "    \n",
    "    imgs_list_full.append(imgs_list_part)\n",
    "    vecs_list_full.append(vecs_list_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_list_full = np.concatenate(np.concatenate(imgs_list_full))\n",
    "vecs_list_full = np.concatenate(np.concatenate(vecs_list_full))\n",
    "\n",
    "# Shuffle the lists. This takes a while\n",
    "np.random.shuffle(imgs_list_full)\n",
    "np.random.shuffle(vecs_list_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits\n",
    "num_samples = len(imgs_list_full)\n",
    "splits = [0.7, 0.2, 0.1]\n",
    "\n",
    "X_img = imgs_list_full[:, 0, ...]\n",
    "X_angle = vecs_list_full[:, 0, ...]\n",
    "y_img = imgs_list_full[:, 1, ...]\n",
    "y_angle = vecs_list_full[:, 1, ...]\n",
    "\n",
    "X_img_train, X_img_valid, X_img_test = np.split(X_img, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])\n",
    "X_angle_train, X_angle_valid, X_angle_test = np.split(X_angle, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])\n",
    "y_img_train, y_img_valid, y_img_test = np.split(y_img, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])\n",
    "y_angle_train, y_angle_valid, y_angle_test = np.split(y_angle, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Cb-oRNU8v8Ik"
   },
   "outputs": [],
   "source": [
    "X_img_train = torch.tensor(X_img_train, dtype=torch.int)\n",
    "X_angle_train = torch.tensor(X_angle_train, dtype=torch.float32)\n",
    "y_img_train = torch.tensor(y_img_train, dtype=torch.int)\n",
    "y_angle_train = torch.tensor(y_angle_train, dtype=torch.int)\n",
    "\n",
    "X_img_valid = torch.tensor(X_img_valid, dtype=torch.int)\n",
    "X_angle_valid = torch.tensor(X_angle_valid, dtype=torch.float32)\n",
    "y_img_valid = torch.tensor(y_img_valid, dtype=torch.int)\n",
    "y_angle_valid = torch.tensor(y_angle_valid, dtype=torch.int)\n",
    "\n",
    "X_img_test = torch.tensor(X_img_test, dtype=torch.int)\n",
    "X_angle_test = torch.tensor(X_angle_test, dtype=torch.float32)\n",
    "y_img_test = torch.tensor(y_img_test, dtype=torch.int)\n",
    "y_angle_test = torch.tensor(y_angle_test, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Uu2HyXd7wgn7"
   },
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "train_dataset = TensorDataset(X_img_train, X_angle_train, y_img_train, y_angle_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = TensorDataset(X_img_valid, X_angle_valid, y_img_valid, y_angle_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = TensorDataset(X_img_test, X_angle_test, y_img_test, y_angle_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                    | 0/100 [00:00<?, ?it/s]C:\\Users\\Codey\\anaconda3\\envs\\unityeyes\\Lib\\site-packages\\torch\\nn\\functional.py:4316: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "  0%|                                                                                               | 0/100 [10:25<?, ?it/s, TLoss=1808587.260, VLoss=403644.784]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loss_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 96\u001b[0m\n\u001b[0;32m     94\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 96\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, \u001b[43mtrain_loss_list\u001b[49m, color \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, markerfacecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     97\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, validation_loss_list, color \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, markerfacecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     99\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks(ticks\u001b[38;5;241m=\u001b[39mepochs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loss_list' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# model = nn.DataParallel(model) # Comment out if using only one GPU\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "weight_correction_loss = 0.8\n",
    "weight_reconstruction_loss = 0.2\n",
    "\n",
    "printout_freq = 1\n",
    "save_ckpt_freq = 1\n",
    "train_checkpoint_path = \"./checkpoints\"\n",
    "start_time = time.time()\n",
    "\n",
    "warp = WarpImageWithFlowAndBrightness(next(iter(train_loader))[0])\n",
    "\n",
    "train_losses, valid_losses, epochs = [], [], []\n",
    "\n",
    "pbar = tqdm(range(num_epochs))\n",
    "for epoch in pbar:\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for imgs, angles, targets, target_angles in train_loader:\n",
    "        imgs, angles, targets, target_angles = (\n",
    "            imgs.float().to(device), angles.float().to(device), targets.float().to(device), target_angles.float().to(device)\n",
    "        )\n",
    "        \n",
    "        flow_corr, bright_corr = model(imgs, target_angles)\n",
    "        img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "        loss_correction = criterion(img_corr, targets)\n",
    "        \n",
    "        flow_reconstruction, bright_reconstruction = model(img_corr, angles)\n",
    "        img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "        loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "        \n",
    "        loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss * loss_reconstruction)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        for imgs, angles, targets, target_angles in valid_loader:\n",
    "            imgs, angles, targets, target_angles = (\n",
    "                imgs.float().to(device), angles.float().to(device), targets.float().to(device), target_angles.float().to(device)\n",
    "            )\n",
    "\n",
    "            flow_corr, bright_corr = model(imgs, target_angles)\n",
    "            img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "            loss_correction = criterion(img_corr, targets)\n",
    "\n",
    "            flow_reconstruction, bright_reconstruction = model(img_corr, angles)\n",
    "            img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "            loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "\n",
    "            loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss * loss_reconstruction)\n",
    "            valid_loss += loss.item()\n",
    "        valid_losses.append(valid_loss / len(valid_loader))\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    overall_time = time.time() - start_time\n",
    "    num_days = int(overall_time / 86400)\n",
    "    num_hrs = int((overall_time-(86400*num_days)) / 3600)\n",
    "    num_mins = int((overall_time-(86400*num_days)-(3600*num_hrs)) / 60)\n",
    "    num_secs = overall_time-(86400*num_days)-(3600*num_hrs)*(60*num_mins)\n",
    "    epochs.append(epoch+1)\n",
    "\n",
    "    \n",
    "    if (epoch + 1) % printout_freq == 0:\n",
    "        pbar.set_postfix({\"TLoss\": f\"{train_loss:.3f}\", \"VLoss\": f\"{valid_loss:.3f}\"})\n",
    "\n",
    "    if (epoch + 1) % save_ckpt_freq == 0:\n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(), # model.state_dict() if using only one GPU ; model.module.state_dict() if parallel\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'validation_loss': valid_loss,\n",
    "            }, os.path.join(train_checkpoint_path, f\"ckpt_{epoch+1}.pt\"))\n",
    "    \n",
    "        # Create loss plot\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.plot(epochs, train_losses, color =\"green\", label=\"Training Loss\", marker='o', markerfacecolor='green')\n",
    "        plt.plot(epochs, valid_losses, color =\"red\", linewidth=1.0, linestyle='--', label=\"Validation Loss\", marker='o', markerfacecolor='red')\n",
    "\n",
    "        plt.xticks(ticks=epochs)\n",
    "        plt.legend()\n",
    "        plt.savefig('./progress/progress_epoch_'+str(epoch+1)+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, angles, targets, target_angles in test_loader:\n",
    "        imgs, angles, targets, target_angles = (\n",
    "            imgs.float().to(device), angles.float().to(device), targets.float().to(device), target_angles.float().to(device)\n",
    "        )\n",
    "\n",
    "        flow_corr, bright_corr = model(imgs, target_angles)\n",
    "        img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "        loss_correction = criterion(img_corr, targets)\n",
    "        \n",
    "        flow_reconstruction, bright_reconstruction = model(img_corr, angles)\n",
    "        img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "        loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "        \n",
    "        loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss * loss_reconstruction)\n",
    "        test_loss += loss.item()\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
