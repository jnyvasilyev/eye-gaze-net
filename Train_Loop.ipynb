{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWVAyg_GsMRN",
    "outputId": "fd891c81-410b-4020-df26-322a6eae0ba1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from ipdb import set_trace\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import model\n",
    "from warp import WarpImageWithFlowAndBrightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_list(json_list):\n",
    "    ldmks = [eval(s) for s in json_list]\n",
    "    return np.array([(x, y, z) for (x, y, z) in ldmks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_info(filename):\n",
    "    info_dict = {}\n",
    "\n",
    "    # Get ID data\n",
    "    titles_types = {'ID': int, 'T': str, 'N': int, 'F': int, 'V': float, 'H': float}\n",
    "    info_list = os.path.basename(filename[:-5]).split('_')\n",
    "    for title, info in zip(titles_types, info_list):\n",
    "        info_dict[title] = titles_types[title](info[len(title):])\n",
    "    info_dict['target'] = info_dict['F'] == 1\n",
    "    \n",
    "    # Get json data\n",
    "    json_data_file = open(filename)\n",
    "    json_data = json.load(json_data_file)\n",
    "    info_dict[\"look_vec\"] = np.array(eval(json_data[\"eye_details\"][\"look_vec\"])) # 3D look direction vector\n",
    "    info_dict[\"interior_margin\"] = process_json_list(json_data[\"interior_margin_2d\"]) # eye interior landmarks\n",
    "    info_dict[\"caruncle\"] = process_json_list(json_data[\"caruncle_2d\"]) # caruncle landmarks\n",
    "    info_dict[\"iris\"] = process_json_list(json_data[\"iris_2d\"]) # iris landmarks\n",
    "    json_data_file.close()\n",
    "    \n",
    "    # Get image data\n",
    "    img = cv2.cvtColor(cv2.imread(\"%s.jpg\" % filename[:-5]), cv2.COLOR_BGR2RGB)\n",
    "    eye_hor_range = np.max(info_dict['interior_margin'], axis=0)[0] - np.min(info_dict['interior_margin'], axis=0)[0]\n",
    "    eye_vert_range = np.max(info_dict['interior_margin'], axis=0)[1] - np.min(info_dict['interior_margin'], axis=0)[1]\n",
    "    if eye_hor_range // 2 > eye_vert_range:\n",
    "        eye_vert_range = eye_hor_range // 2\n",
    "    else:\n",
    "        eye_hor_range = eye_vert_range * 2\n",
    "    eyeball_center = np.array([300, 400])\n",
    "    range_coef = 2\n",
    "    img_cropped = img[eyeball_center[0] - int(eye_vert_range // 2 * range_coef) : eyeball_center[0] + int(eye_vert_range // 2 * range_coef),\n",
    "                      eyeball_center[1] - int(eye_hor_range // 2 * range_coef) : eyeball_center[1] + int(eye_hor_range // 2 * range_coef)]\n",
    "    img = cv2.resize(img_cropped, (64, 32))\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    info_dict['img'] = img\n",
    "    \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = '../dataset/UnityEyes_Windows/imgs_2_cutouts/ID1_Tgaussian_N001_F1_V0.00_H0.02.json'\n",
    "info_dict = get_filename_info(test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the dataframe info\n",
    "- look_vec is a 3D homogeneous vector in the form \\[x, y, z, 0\\]. For purposes of the model input, only the x and y components are needed. I believe this vector is already normalized, but we can renormalize this vector before extracting the x and y components\n",
    "- Feature landmarks (i.e., interior_margin, caruncle, and iris) are the pixel coordinates of feature BEFORE RESIZING. Ideally they are used to determine the crop/resize area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "The above dataframe contains info on all images, separated into IDs. As our model input, we would like to take all possible image pairs within an ID. With 40 images per ID, this results in 780 pairs.\n",
    "\n",
    "Dataloader will output input image, input vector, target image, target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset folder\n",
    "def get_dataset(input_file_path, input_fn):\n",
    "    \"\"\"\n",
    "    Read the image and json data in the specified folder.\n",
    "    Args:\n",
    "        input_file_path: the base directory containing the dataset directories\n",
    "        input_fn: the name of dataset directory\n",
    "    Return:\n",
    "        imgs_list (list): List of images, cropped\n",
    "        vecs_list (list): List of gaze vectors, tiled\n",
    "    \"\"\"\n",
    "    img_infos = []\n",
    "    json_fns = glob(os.path.join(input_file_path, input_fn, \"*.json\"))\n",
    "    for json_fn in json_fns:\n",
    "        info = get_filename_info(json_fn)\n",
    "        img_infos.append(info)\n",
    "        \n",
    "    img_df = pd.DataFrame(img_infos)\n",
    "    img_df.sort_values(['ID', 'F'], ignore_index=True, inplace=True)\n",
    "\n",
    "    # Extract relevant data for dataloader\n",
    "    n_ids = img_df.iloc[-1]['ID']\n",
    "    \n",
    "    imgs_list = []\n",
    "    vecs_list = []\n",
    "    \n",
    "    # For each ID, generate all possible pairs\n",
    "    for id in range(1, n_ids + 1):\n",
    "        # Get ID\n",
    "        df_chunk = img_df.query(f\"ID == {id}\")\n",
    "        imgs = np.stack(df_chunk['img'])\n",
    "        vecs = np.stack(df_chunk['look_vec'].to_numpy())\n",
    "        vecs = (vecs / np.linalg.norm(vecs, axis=1, keepdims=True))[:, :2] # normalize, then get x and y components\n",
    "        vecs = np.tile(vecs[:, :, np.newaxis, np.newaxis], (1, 1, 32, 64))\n",
    "    \n",
    "        # Generate pairs\n",
    "        pairs = np.triu_indices(len(df_chunk), k=1)\n",
    "        pairs = np.stack(pairs).transpose()\n",
    "    \n",
    "        img_pairs = imgs[pairs] # (pair_idx, input or output, C, H, W)\n",
    "        vec_pairs = vecs[pairs] # (pair_idx, input or output, x or y, ...)\n",
    "    \n",
    "        imgs_list.append(img_pairs)\n",
    "        vecs_list.append(vec_pairs)\n",
    "    \n",
    "    return imgs_list, vecs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "\n",
    "imgs_list_full = []\n",
    "vecs_list_full = []\n",
    "input_filename_list = ['imgs_1_cutouts', 'imgs_2_cutouts', 'imgs_3_cutouts', 'imgs_4_cutouts']\n",
    "input_file_path = os.path.join(os.getcwd(), '..', 'dataset', 'UnityEyes_Windows')\n",
    "\n",
    "print(\"Reading dataset\")\n",
    "for input_fn in tqdm(input_filename_list):\n",
    "    print(\"Reading \" + input_fn)\n",
    "    imgs_list_part, vecs_list_part = get_dataset(input_file_path, input_fn)\n",
    "    \n",
    "    imgs_list_full.append(imgs_list_part)\n",
    "    vecs_list_full.append(vecs_list_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_list_full = np.concatenate(np.concatenate(imgs_list_full))\n",
    "vecs_list_full = np.concatenate(np.concatenate(vecs_list_full))\n",
    "\n",
    "# Shuffle the lists. This takes a while\n",
    "np.random.shuffle(imgs_list_full)\n",
    "np.random.shuffle(vecs_list_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits\n",
    "num_samples = len(imgs_list_full)\n",
    "splits = [0.7, 0.2, 0.1]\n",
    "\n",
    "X_img = imgs_list_full[:, 0, ...]\n",
    "X_angle = vecs_list_full[:, 0, ...]\n",
    "y_img = imgs_list_full[:, 1, ...]\n",
    "y_angle = vecs_list_full[:, 1, ...]\n",
    "\n",
    "X_img_train, X_img_valid, X_img_test = np.split(X_img, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])\n",
    "X_angle_train, X_angle_valid, X_angle_test = np.split(X_angle, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])\n",
    "y_img_train, y_img_valid, y_img_test = np.split(y_img, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])\n",
    "y_angle_train, y_angle_valid, y_angle_test = np.split(y_angle, [int(num_samples * splits[0]), int(num_samples * (splits[0] + splits[1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Cb-oRNU8v8Ik"
   },
   "outputs": [],
   "source": [
    "X_img_train = torch.tensor(X_img_train, dtype=torch.int)\n",
    "X_angle_train = torch.tensor(X_angle_train, dtype=torch.float32)\n",
    "y_img_train = torch.tensor(y_img_train, dtype=torch.int)\n",
    "y_angle_train = torch.tensor(y_angle_train, dtype=torch.int)\n",
    "\n",
    "X_img_valid = torch.tensor(X_img_valid, dtype=torch.int)\n",
    "X_angle_valid = torch.tensor(X_angle_valid, dtype=torch.float32)\n",
    "y_img_valid = torch.tensor(y_img_valid, dtype=torch.int)\n",
    "y_angle_valid = torch.tensor(y_angle_valid, dtype=torch.int)\n",
    "\n",
    "X_img_test = torch.tensor(X_img_test, dtype=torch.int)\n",
    "X_angle_test = torch.tensor(X_angle_test, dtype=torch.float32)\n",
    "y_img_test = torch.tensor(y_img_test, dtype=torch.int)\n",
    "y_angle_test = torch.tensor(y_angle_test, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Uu2HyXd7wgn7"
   },
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "train_dataset = TensorDataset(X_img_train, X_angle_train, y_img_train, y_angle_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = TensorDataset(X_img_valid, X_angle_valid, y_img_valid, y_angle_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = TensorDataset(X_img_test, X_angle_test, y_img_test, y_angle_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# model = nn.DataParallel(model) # Comment out if using only one GPU\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "weight_correction_loss = 0.8\n",
    "weight_reconstruction_loss = 0.2\n",
    "\n",
    "printout_freq = 1\n",
    "save_ckpt_freq = 25\n",
    "train_checkpoint_path = \"./checkpoints\"\n",
    "start_time = time.time()\n",
    "\n",
    "warp = WarpImageWithFlowAndBrightness(next(iter(train_loader))[0])\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "pbar = tqdm(range(num_epochs))\n",
    "for epoch in pbar:\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for imgs, angles, targets, target_angles in train_loader:\n",
    "        imgs, angles, targets, target_angles = (\n",
    "            imgs.float().to(device), angles.float().to(device), targets.float().to(device), target_angles.float().to(device)\n",
    "        )\n",
    "        \n",
    "        flow_corr, bright_corr = model(imgs, target_angles)\n",
    "        img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "        loss_correction = criterion(img_corr, targets)\n",
    "        \n",
    "        flow_reconstruction, bright_reconstruction = model(img_corr, angles)\n",
    "        img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "        loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "        \n",
    "        loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss * loss_reconstruction)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        for imgs, angles, targets, target_angles in valid_loader:\n",
    "            imgs, angles, targets, target_angles = (\n",
    "                imgs.float().to(device), angles.float().to(device), targets.float().to(device), target_angles.float().to(device)\n",
    "            )\n",
    "\n",
    "            flow_corr, bright_corr = model(imgs, target_angles)\n",
    "            img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "            loss_correction = criterion(img_corr, targets)\n",
    "\n",
    "            flow_reconstruction, bright_reconstruction = model(img_corr, angles)\n",
    "            img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "            loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "\n",
    "            loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss * loss_reconstruction)\n",
    "            valid_loss += loss.item()\n",
    "        valid_losses.append(valid_loss / len(valid_loader))\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    overall_time = time.time() - start_time\n",
    "    num_days = int(overall_time / 86400)\n",
    "    num_hrs = int((overall_time-(86400*num_days)) / 3600)\n",
    "    num_mins = int((overall_time-(86400*num_days)-(3600*num_hrs)) / 60)\n",
    "    num_secs = overall_time-(86400*num_days)-(3600*num_hrs)*(60*num_mins)\n",
    "    if (epoch + 1) % printout_freq == 0:\n",
    "    #     print(f\"\"\"Finished epoch {epoch + 1}/{num_epochs} ({(epoch+1)/num_epochs*100:.2f}%)\n",
    "    #            total time is {num_days}d:{num_hrs}h:{num_mins}m:{num_secs:.3f}s; time for this epoch is {epoch_time:.2f}s\n",
    "    #            training loss was {bcolors.BOLD}{train_loss:.3f}{bcolors.ENDC}, validation_loss was {bcolors.BOLD}{valid_loss:.3f}{bcolors.ENDC}.\"\"\")\n",
    "        pbar.set_postfix({\"TLoss\": f\"{train_loss:.3f}\", \"VLoss\": f\"{valid_loss:.3f}\"})\n",
    "\n",
    "    if (epoch + 1) % save_ckpt_freq == 0;\n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(), # model.state_dict() if using only one GPU ; model.module.state_dict() if parallel\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'validation_loss': valid_loss,\n",
    "            }, train_checkpoint_path)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, angles, targets, target_angles in test_loader:\n",
    "        imgs, angles, targets, target_angles = (\n",
    "            imgs.float().to(device), angles.float().to(device), targets.float().to(device), target_angles.float().to(device)\n",
    "        )\n",
    "\n",
    "        flow_corr, bright_corr = model(imgs, target_angles)\n",
    "        img_corr = warp(imgs, flow_corr, bright_corr)\n",
    "        loss_correction = criterion(img_corr, targets)\n",
    "        \n",
    "        flow_reconstruction, bright_reconstruction = model(img_corr, angles)\n",
    "        img_reconstruction = warp(img_corr, flow_reconstruction, bright_reconstruction)\n",
    "        loss_reconstruction = criterion(img_reconstruction, imgs)\n",
    "        \n",
    "        loss = (weight_correction_loss * loss_correction) + (weight_reconstruction_loss * loss_reconstruction)\n",
    "        test_loss += loss.item()\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
